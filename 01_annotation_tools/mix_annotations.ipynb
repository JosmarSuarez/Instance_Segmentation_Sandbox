{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mix_annotations.ipynb\n",
    "Combines coco .json files into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import json files by a dict of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def read_annotations(ann_paths):\n",
    "    #Create empty list to store the datasets info\n",
    "    my_dicts = []\n",
    "    #Iterate for every dataset path\n",
    "    for ann_path in ann_paths:\n",
    "        # Read the .json file\n",
    "        with open(ann_path) as f:\n",
    "            data = json.load(f)\n",
    "            my_dicts.append(data)\n",
    "            print(\"Completed: \", ann_path )\n",
    "    return my_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix annotations from the loaded dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def mix_annotations(my_dicts):\n",
    "    combined_dict = copy.deepcopy(my_dicts[0])\n",
    "    im_id = len(my_dicts[0][\"images\"])\n",
    "    ann_id = len(my_dicts[0][\"annotations\"])\n",
    "    # print(im_id, ann_id)\n",
    "    for d in my_dicts[1:]:\n",
    "        for image in d[\"images\"]:\n",
    "            temp = image.copy()\n",
    "            temp[\"id\"] = im_id + temp[\"id\"] \n",
    "            combined_dict[\"images\"].append(temp)\n",
    "        for ann in d[\"annotations\"]:\n",
    "            temp = ann.copy()\n",
    "            temp[\"id\"] = ann_id + temp[\"id\"]\n",
    "            temp[\"image_id\"] = im_id + temp[\"image_id\"] \n",
    "            combined_dict[\"annotations\"].append(temp)\n",
    "\n",
    "        im_id = len(d[\"images\"])\n",
    "        ann_id = len(d[\"annotations\"])\n",
    "    return combined_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing images example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Completed:  /home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/filtered_mixed_ucb_gait.json\nCompleted:  /home/josmar/proyectos/codes/datasets/ucb_gait_frames/annotations/filtered_ucb_gait_poly.json\n"
    }
   ],
   "source": [
    "ann_paths = [\n",
    "    \"/home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/filtered_mixed_ucb_gait.json\",\n",
    "    \"/home/josmar/proyectos/codes/datasets/ucb_gait_frames/annotations/filtered_ucb_gait_poly.json\",\n",
    "    ]\n",
    "my_dicts= read_annotations(ann_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nDataset  0\nImage IDs:\t 10000\nAnnotation IDs:\t 20000\n\nDataset  1\nImage IDs:\t 11750\nAnnotation IDs:\t 11750\n"
    }
   ],
   "source": [
    "index = 0\n",
    "for d in my_dicts:\n",
    "    print(\"\\nDataset \",index)\n",
    "    print(\"Image IDs:\\t\" , len(d[\"images\"]))\n",
    "    print(\"Annotation IDs:\\t\" , len(d[\"annotations\"]))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = mix_annotations(my_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nCombined dataset\nImage IDs:\t 21750\nAnnotation IDs:\t 31750\n"
    }
   ],
   "source": [
    "print(\"\\nCombined dataset\")\n",
    "print(\"Image IDs:\\t\" , len(combined_dict[\"images\"]))\n",
    "print(\"Annotation IDs:\\t\" , len(combined_dict[\"annotations\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/josmar/proyectos/codes/datasets/ucb_gait_combined/annotations/ucb_gait_combined.json\"\n",
    "with open(out_path, 'w') as fp:\n",
    "    json.dump(combined_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train Test and Val datasets with the generated json\n",
    "Based on https://github.com/akarazniewicz/cocosplit/blob/master/cocosplit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coco(file, info, licenses, images, annotations, categories):\n",
    "    with open(file, 'wt', encoding='UTF-8') as coco:\n",
    "        json.dump({ 'info': info, 'licenses': licenses, 'images': images, \n",
    "            'annotations': annotations, 'categories': categories}, coco, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations(annotations, images):\n",
    "    image_ids = funcy.lmap(lambda i: int(i['id']), images)\n",
    "    return funcy.lfilter(lambda a: int(a['image_id']) in image_ids, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved\n         17400 entries in /home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_train_ucb_gait.json\n         2175 entries in /home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_val_ucb_gait.json\n         2175 entries in /home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_test_ucb_gait.json\n"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import funcy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "out_path = \"/home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_images_ucb.json\"\n",
    "has_annotations = True\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "train_file = \"/home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_train_ucb_gait.json\"\n",
    "val_file = \"/home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_val_ucb_gait.json\"\n",
    "test_file = \"/home/josmar/proyectos/codes/01_annotation_tools/test_data/annotations/filtered/cropped_test_ucb_gait.json\"\n",
    "with open(out_path, 'rt', encoding='UTF-8') as annotations:\n",
    "    \n",
    "    coco = json.load(annotations)\n",
    "    info = coco['info']\n",
    "    licenses = coco['licenses']\n",
    "    images = coco['images']\n",
    "    annotations = coco['annotations']\n",
    "    categories = coco['categories']\n",
    "\n",
    "    number_of_images = len(images)\n",
    "\n",
    "    images_with_annotations = funcy.lmap(lambda a: int(a['image_id']), annotations)\n",
    "\n",
    "    if has_annotations:\n",
    "        images = funcy.lremove(lambda i: i['id'] not in images_with_annotations, images)\n",
    "\n",
    "    x, y = train_test_split(images, train_size=train_split, shuffle=True)\n",
    "\n",
    "    val_split = round(val_split/(1-train_split) , 2)\n",
    "    y, z = train_test_split(y, train_size=val_split, shuffle=True)\n",
    "    \n",
    "    save_coco(train_file, info, licenses, x, filter_annotations(annotations, x), categories)\n",
    "    save_coco(val_file, info, licenses, y, filter_annotations(annotations, y), categories)\n",
    "    save_coco(test_file, info, licenses, z, filter_annotations(annotations, z), categories)\n",
    "\n",
    "    print(\"Saved\\n \\\n",
    "        {} entries in {}\\n \\\n",
    "        {} entries in {}\\n \\\n",
    "        {} entries in {}\".format(len(x), train_file, len(y), val_file, len(z), test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5\n"
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "print(val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ORDERED Train Test and Val datasets with the generated json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"/home/josmar/proyectos/codes/datasets/ucb_gait_cropped/cropped_images_ucb.json\"\n",
    "\n",
    "with open(dataset_path) as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['info', 'licenses', 'images', 'categories', 'annotations'])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total images:  21750\nTotal backgrounds:  33 \n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "    Background NÂ° images  Percentage\n0        abbey       277        1.27\n1       basket       288        1.32\n2        beach       292        1.34\n3   buildings2       328        1.51\n4   buildings3       316        1.45\n5    buildings       289        1.33\n6       coffee       314        1.44\n7      curtain       293        1.35\n8      desert2       314        1.44\n9       desert       302        1.39\n10    elevator       310        1.43\n11        fall       288        1.32\n12      forest       306        1.41\n13         lab     12048       55.39\n14   monastery       291        1.34\n15         out       293        1.35\n16        park       318        1.46\n17    parking2       277        1.27\n18    parking3       302        1.39\n19     parking       304        1.40\n20        road       275        1.26\n21        room       300        1.38\n22       snow2       317        1.46\n23        snow       330        1.52\n24       stage       306        1.41\n25     street2       330        1.52\n26     street3       314        1.44\n27      street       311        1.43\n28         toy       305        1.40\n29       train       276        1.27\n30      valley       295        1.36\n31        wall       309        1.42\n32  whitehouse       332        1.53",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Background</th>\n      <th>NÂ° images</th>\n      <th>Percentage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abbey</td>\n      <td>277</td>\n      <td>1.27</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>basket</td>\n      <td>288</td>\n      <td>1.32</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beach</td>\n      <td>292</td>\n      <td>1.34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>buildings2</td>\n      <td>328</td>\n      <td>1.51</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>buildings3</td>\n      <td>316</td>\n      <td>1.45</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>buildings</td>\n      <td>289</td>\n      <td>1.33</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>coffee</td>\n      <td>314</td>\n      <td>1.44</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>curtain</td>\n      <td>293</td>\n      <td>1.35</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>desert2</td>\n      <td>314</td>\n      <td>1.44</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>desert</td>\n      <td>302</td>\n      <td>1.39</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>elevator</td>\n      <td>310</td>\n      <td>1.43</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fall</td>\n      <td>288</td>\n      <td>1.32</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>forest</td>\n      <td>306</td>\n      <td>1.41</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>lab</td>\n      <td>12048</td>\n      <td>55.39</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>monastery</td>\n      <td>291</td>\n      <td>1.34</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>out</td>\n      <td>293</td>\n      <td>1.35</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>park</td>\n      <td>318</td>\n      <td>1.46</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>parking2</td>\n      <td>277</td>\n      <td>1.27</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>parking3</td>\n      <td>302</td>\n      <td>1.39</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>parking</td>\n      <td>304</td>\n      <td>1.40</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>road</td>\n      <td>275</td>\n      <td>1.26</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>room</td>\n      <td>300</td>\n      <td>1.38</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>snow2</td>\n      <td>317</td>\n      <td>1.46</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>snow</td>\n      <td>330</td>\n      <td>1.52</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>stage</td>\n      <td>306</td>\n      <td>1.41</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>street2</td>\n      <td>330</td>\n      <td>1.52</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>street3</td>\n      <td>314</td>\n      <td>1.44</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>street</td>\n      <td>311</td>\n      <td>1.43</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>toy</td>\n      <td>305</td>\n      <td>1.40</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>train</td>\n      <td>276</td>\n      <td>1.27</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>valley</td>\n      <td>295</td>\n      <td>1.36</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>wall</td>\n      <td>309</td>\n      <td>1.42</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>whitehouse</td>\n      <td>332</td>\n      <td>1.53</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "bg_dict = {}\n",
    "for image in dataset[\"images\"]:\n",
    "    place = image[\"file_name\"].split(\"_\")[1]\n",
    "    if place.find(\"-\") != -1:\n",
    "        place = \"lab\"\n",
    "    if place in bg_dict:\n",
    "        bg_dict[place]+=1\n",
    "    else:\n",
    "        bg_dict[place]=1\n",
    "total = sum(bg_dict.values()) \n",
    "print(\"Total images: \",total)\n",
    "print(\"Total backgrounds: \", len(bg_dict), \"\\n\")\n",
    "\n",
    "idx=0\n",
    "df = pd.DataFrame(columns=('Background', 'NÂ° images', 'Percentage'))\n",
    "for key, value in bg_dict.items():\n",
    "\n",
    "    percent = round(value/total*100,2)\n",
    "    df.loc[idx] = [key, value, percent]\n",
    "    idx+=1\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['abbey', 'basket', 'beach', 'buildings2', 'buildings3', 'buildings', 'coffee', 'curtain', 'desert2', 'desert', 'elevator', 'fall', 'forest', 'lab', 'monastery', 'out', 'park', 'parking2', 'parking3', 'parking', 'road', 'room', 'snow2', 'snow', 'stage', 'street2', 'street3', 'street', 'toy', 'train', 'valley', 'wall', 'whitehouse'])\n"
    }
   ],
   "source": [
    "print(bg_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bg = ['abbey', 'basket', 'beach', 'buildings', 'curtain', 'desert', 'forest', 'lab', 'monastery', 'out', 'parking', 'snow', 'stage', 'street', 'toy', 'train', 'whitehouse']\n",
    "eval_bg = ['buildings2', 'parking2', 'street2', 'desert2', 'fall', 'elevator', 'park', 'wall']\n",
    "test_bg = ['buildings3', 'parking3', 'street3', 'snow2', 'valley', 'coffee', 'road', 'room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'license': 0,\n 'file_name': 'crop_abbey_002-009_032-227.jpg',\n 'width': 512,\n 'height': 512,\n 'id': 0}"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset[\"images\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we store the images per type inside dataset_images and create a mapping list called dataset_convert that will help us to distribute the annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_dict = {}\n",
    "bg_list = [train_bg, eval_bg, test_bg]\n",
    "dataset_images = [[],[],[]]\n",
    "dataset_convert = [[],[],[]]\n",
    "counter_images = [0,0,0]\n",
    "for image in dataset[\"images\"]:\n",
    "    place = image[\"file_name\"].split(\"_\")[1]\n",
    "    if place.find(\"-\") != -1:\n",
    "        place = \"lab\"\n",
    "    for idx in range(len(bg_list)):\n",
    "        if place in bg_list[idx]:\n",
    "            current_id = image[\"id\"]\n",
    "            new_image = dict(image)\n",
    "            new_image[\"id\"] = counter_images[idx]\n",
    "            dataset_images[idx].append(new_image)\n",
    "            dataset_convert[idx].append([current_id, counter_images[idx]])\n",
    "            counter_images[idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a list of the original values for the dataset images for each dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_originals = []\n",
    "for conv in dataset_convert:\n",
    "    transposed = list(zip(*conv))\n",
    "    convert_originals.append(transposed[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3670\n"
    }
   ],
   "source": [
    "print(convert_originals[0][1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21750\n21750\n"
    }
   ],
   "source": [
    "print(len(dataset_images[0])+len(dataset_images[1])+len(dataset_images[2]))\n",
    "print(len(dataset_convert[0])+len(dataset_convert[1])+len(dataset_convert[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'license': 0, 'file_name': 'crop_forest_012-174_040-239.jpg', 'width': 512, 'height': 512, 'id': 1800}\n[3670, 1800]\n"
    }
   ],
   "source": [
    "idx=1800\n",
    "print(dataset_images[0][idx])\n",
    "print(dataset_convert[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25349"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len (dataset[\"annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_annotations=[0,0,0]\n",
    "dataset_anns = [[],[],[]]\n",
    "for ann in dataset[\"annotations\"]:\n",
    "    for idx in range(len(dataset_convert)):\n",
    "        if ann[\"image_id\"] in convert_originals[idx]:\n",
    "            i = convert_originals[idx].index(ann[\"image_id\"])\n",
    "            new_ann = dict(ann)\n",
    "            new_ann[\"image_id\"] = i\n",
    "            new_ann[\"id\"]=counter_annotations[idx]\n",
    "            dataset_anns[idx].append(new_ann)\n",
    "            counter_annotations[idx]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "len(dataset_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [[],[],[]]\n",
    "real_image_ids = [[],[],[]]\n",
    "for idx in range(len(dataset_images)):\n",
    "    for ann in dataset_anns[idx]:\n",
    "        image_ids[idx].append(ann[\"image_id\"])\n",
    "    for img in dataset_images[idx]:\n",
    "        real_image_ids[idx].append(img[\"id\"])\n",
    "image_ids = [list(set(x)) for x in image_ids]\n",
    "real_image_ids = [list(set(x)) for x in real_image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wrong images: 0\n"
    }
   ],
   "source": [
    "wrong_counter=0\n",
    "for idx in range (len(real_image_ids)):\n",
    "    for im_id in image_ids[idx]:\n",
    "        if not im_id in real_image_ids[idx]:\n",
    "            wrong_counter+=1\n",
    "print(\"Wrong images:\", wrong_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "a=[2,4,6,8]\n",
    "b=3\n",
    "not b in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dataset_titles = [\"Reordered Train ucb-gait crop\", \"Reordered Val ucb-gait crop\", \"Reordered Test ucb-gait crop\"]\n",
    "dataset_dicts = []\n",
    "\n",
    "for idx in range(len(dataset_images)):\n",
    "    new_dict = dict(dataset)\n",
    "    new_dict[\"info\"][\"description\"]=dataset_titles[idx]\n",
    "    new_dict[\"annotations\"] = dataset_anns[idx]\n",
    "    new_dict[\"images\"] = dataset_images[idx]\n",
    "\n",
    "    dataset_dicts.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "dataset_files = [\"/home/josmar/proyectos/codes/datasets/ucb_gait_cropped/train_reordered_ucb-gait_crop.json\",\n",
    "                    \"/home/josmar/proyectos/codes/datasets/ucb_gait_cropped/val_reordered_ucb-gait_crop.json\",\n",
    "                    \"/home/josmar/proyectos/codes/datasets/ucb_gait_cropped/test_reordered_ucb-gait_crop.json\"]\n",
    "for idx in range(len(dataset_dicts)):\n",
    "    with open(dataset_files[idx], 'w') as json_file:\n",
    "        json.dump(dataset_dicts[idx], json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'description': 'crop_ucb_gait',\n 'url': '',\n 'version': '0.1',\n 'year': 2020,\n 'contributor': 'Josmar Suarez',\n 'date_created': '2020/07/14'}"
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "source": [
    "a[\"info\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitd2condafcc7f242a0d248929c8f8110917217d8",
   "display_name": "Python 3.6.10 64-bit ('d2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}