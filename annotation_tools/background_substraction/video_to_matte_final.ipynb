{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folder and video to images dependencies\n",
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "# Segmentation dependencies\n",
    "import sys\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2, pdb, glob, argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#importing numba to clean all memory allocated\n",
    "from numba import cuda\n",
    "\n",
    "#Matting dependencies\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "import glob, time, argparse, pdb\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.measure import label\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from functions import *\n",
    "from networks import ResnetConditionHR\n",
    "\n",
    "# Import time to measure time of execution\n",
    "import time\n",
    "\n",
    "# Import json to write times\n",
    "import json\n",
    "#Tensorflow doesnt release memory after executed(NO used)\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating video in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video2image(input_video, size, input_folder):\n",
    "    FFMPEG_BIN = \"ffmpeg\"\n",
    "    command = [ FFMPEG_BIN,\n",
    "                '-i', input_video, \n",
    "                '-vf', 'scale={}:{}'.format(size[0],size[1]),\n",
    "                os.path.join(input_folder,\"%04d_img.png\")]\n",
    "    print(\"Extracting frames from video into /input folder...\")\n",
    "    sp.run(command)\n",
    "    print(\"Extraction finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running segmentation\n",
    "Using deeplabv3 to segment the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        #\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "\n",
    "        tar_file.close()\n",
    "\n",
    "        if graph_def is None:\n",
    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "        #I added this code\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        #----------------\n",
    "        self.sess = tf.Session(config=config,graph=self.graph)\n",
    "\n",
    "    def run(self, image):\n",
    "        \"\"\"Runs inference on a single image.\n",
    "\n",
    "        Args:\n",
    "          image: A PIL.Image object, raw input image.\n",
    "\n",
    "        Returns:\n",
    "          resized_image: RGB image resized from original input image.\n",
    "          seg_map: Segmentation map of `resized_image`.\n",
    "        \"\"\"\n",
    "        width, height = image.size\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return resized_image, seg_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_pascal_label_colormap():\n",
    "    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
    "\n",
    "    Returns:\n",
    "    A Colormap for visualizing segmentation results.\n",
    "    \"\"\"\n",
    "    colormap = np.zeros((256, 3), dtype=int)\n",
    "    ind = np.arange(256, dtype=int)\n",
    "\n",
    "    for shift in reversed(range(8)):\n",
    "        for channel in range(3):\n",
    "          colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "        ind >>= 3\n",
    "\n",
    "    return colormap\n",
    "\n",
    "def label_to_color_image(label):\n",
    "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "    Args:\n",
    "    label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "    Returns:\n",
    "    result: A 2D array with floating type. The element of the array\n",
    "      is the color indexed by the corresponding element in the input label\n",
    "      to the PASCAL color map.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If label is not of rank 2 or its value is larger than color\n",
    "      map maximum entry.\n",
    "    \"\"\"\n",
    "    if label.ndim != 2:\n",
    "        raise ValueError('Expect 2-D input label')\n",
    "\n",
    "    colormap = create_pascal_label_colormap()\n",
    "\n",
    "    if np.max(label) >= len(colormap):\n",
    "        raise ValueError('label value too large.')\n",
    "\n",
    "    return colormap[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def deeplab_segmentation(input_folder, include_empty=True, total_frames = -1, back_img_path=None):\n",
    "    dir_name = input_folder\n",
    "    ## setup ####################\n",
    "\n",
    "    LABEL_NAMES = np.asarray([\n",
    "        'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "        'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "        'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "    ])\n",
    "\n",
    "    FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "    FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
    "\n",
    "\n",
    "    MODEL_NAME = 'xception_coco_voctrainval'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n",
    "\n",
    "    _DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "    _MODEL_URLS = {\n",
    "        'mobilenetv2_coco_voctrainaug':\n",
    "            'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "        'mobilenetv2_coco_voctrainval':\n",
    "            'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
    "        'xception_coco_voctrainaug':\n",
    "            'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
    "        'xception_coco_voctrainval':\n",
    "            'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
    "    }\n",
    "    _TARBALL_NAME = _MODEL_URLS[MODEL_NAME]\n",
    "\n",
    "    model_dir = 'deeplab_model'\n",
    "    if not os.path.exists(model_dir):\n",
    "        tf.gfile.MakeDirs(model_dir)\n",
    "\n",
    "    download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
    "    if not os.path.exists(download_path):\n",
    "        print('downloading model to %s, this might take a while...' % download_path)\n",
    "        urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], \n",
    "                        download_path)\n",
    "        print('download completed! loading DeepLab model...')\n",
    "\n",
    "    MODEL = DeepLabModel(download_path)\n",
    "    print('model loaded successfully!')\n",
    "\n",
    "    #######################################################################################\n",
    "\n",
    "    list_im=glob.glob(dir_name + '/*_img.png'); list_im.sort()\n",
    "    #Creating useful frame counter\n",
    "    if total_frames == -1:\n",
    "       total_frames = len(list_im)\n",
    "\n",
    "    count = 0\n",
    "    # variables to create back img\n",
    "    count_empty = 0\n",
    "    isCreated = False\n",
    "    #--------------------- \n",
    "    for i in range(0,len(list_im)):\n",
    "        \n",
    "        if count >= total_frames:\n",
    "            break \n",
    "        image = Image.open(list_im[i])\n",
    "\n",
    "        res_im,seg=MODEL.run(image)\n",
    "\n",
    "        seg=cv2.resize(seg.astype(np.uint8),image.size)\n",
    "\n",
    "        mask_sel=(seg==15).astype(np.float32)\n",
    "\n",
    "        name=list_im[i].replace('img','masksDL')\n",
    "        out_mask = (255*mask_sel).astype(np.uint8)\n",
    "        \n",
    "        if(cv2.countNonZero(out_mask)>15 or include_empty):\n",
    "            cv2.imwrite(name, out_mask)\n",
    "            count+=1\n",
    "            count_empty=0 #reset back_img_counter\n",
    "        else:\n",
    "            count_empty+=1 #back_img_counter\n",
    "            if(count_empty > 8 and not isCreated and back_img_path != None):\n",
    "                shutil.copy(list_im[i], back_img_path)\n",
    "                isCreated = True\n",
    "                print('\\tbackground image extracted')\n",
    "        sys.stdout.write('\\r'+  \"Done: {}/{}    Frames: {}/{}\".format(i+1,len(list_im), count, total_frames))\n",
    "        sys.stdout.flush()\n",
    "    str_msg='\\nDone: ' + dir_name\n",
    "    print(str_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open('/home/josmar/proyectos/Background-Matting/walking_clips/temp/1280x720/input/0001_img.png')\n",
    "# # image.save(\"walking_clips/back_mario_1280x720.png\")\n",
    "# shutil.copy('/home/josmar/proyectos/Background-Matting/walking_clips/temp/1280x720/input/0001_img.png',\"walking_clips/back_mario_1280x720.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cleaning the allocated memory\n",
    "def clean_memory():\n",
    "    device = cuda.get_current_device()\n",
    "    device.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the background matte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def obtain_matting(input_folder, output_folder, back_path, trained_model, include_empty=True, total_frames = -1):    \n",
    "    torch.set_num_threads(1)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "    print('CUDA Device: ' + os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "\n",
    "    # \"\"\"Parses arguments.\"\"\"\n",
    "    # parser = argparse.ArgumentParser(description='Background Matting.')\n",
    "    # parser.add_argument('-m', '--trained_model', type=str, default='real-fixed-cam',choices=['real-fixed-cam', 'real-hand-held', 'syn-comp-adobe'],help='Trained background matting model')\n",
    "    # parser.add_argument('-o', '--output_dir', type=str, required=True,help='Directory to save the output results. (required)')\n",
    "    # parser.add_argument('-i', '--input_dir', type=str, required=True,help='Directory to load input images. (required)')\n",
    "    # parser.add_argument('-tb', '--target_back', type=str,help='Directory to load the target background.')\n",
    "    # parser.add_argument('-b', '--back', type=str,default=None,help='Captured background image. (only use for inference on videos with fixed camera')\n",
    "\n",
    "\n",
    "    # args=parser.parse_args()\n",
    "\n",
    "    #input model\n",
    "    model_main_dir='Models/' + trained_model + '/';\n",
    "    #input data path\n",
    "    data_path= input_folder\n",
    "\n",
    "    is_video=True\n",
    "    print('Using video mode')\n",
    "\n",
    "\n",
    "    #initialize network\n",
    "    fo=glob.glob(model_main_dir + 'netG_epoch_*')\n",
    "    model_name1=fo[0]\n",
    "    netM=ResnetConditionHR(input_nc=(3,3,1,4),output_nc=4,n_blocks1=7,n_blocks2=3)\n",
    "    netM=nn.DataParallel(netM)\n",
    "    netM.load_state_dict(torch.load(model_name1))\n",
    "    netM.cuda(); netM.eval()\n",
    "    cudnn.benchmark=True\n",
    "    reso=(512,512) #input reoslution to the network\n",
    "\n",
    "    #load captured background for video mode, fixed camera\n",
    "    if back_path is not None:\n",
    "        bg_im0=cv2.imread(back_path); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB);\n",
    "\n",
    "\n",
    "    \n",
    "    #Create a list of test masks (changed to read only segmentation masks)\n",
    "    test_masks = [f for f in os.listdir(data_path) if\n",
    "                os.path.isfile(os.path.join(data_path, f)) and f.endswith('_masksDL.png')]\n",
    "    #Create a list of test images\n",
    "    test_imgs = [name.replace('_masksDL','_img') for name in test_masks]\n",
    "\n",
    "    test_imgs.sort()\n",
    "    test_masks.sort()\n",
    "    \n",
    "    #Limit the number of frames\n",
    "    if total_frames ==-1:\n",
    "        total_frames = len(test_imgs)\n",
    "\n",
    "    #output directory\n",
    "    result_path= output_folder\n",
    "\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    \n",
    "    #Creating number of frames processed counter\n",
    "    count= 0\n",
    "\n",
    "    for i in range(0,len(test_masks)):\n",
    "        \n",
    "        #Close the loop when the number of frames desired is reached\n",
    "        if count >= total_frames:\n",
    "            break\n",
    "        \n",
    "        #read image\n",
    "        filename = test_imgs[i]    \n",
    "        \n",
    "        #original image\n",
    "        bgr_img = cv2.imread(os.path.join(data_path, filename)); bgr_img=cv2.cvtColor(bgr_img,cv2.COLOR_BGR2RGB);\n",
    "\n",
    "        if back_path is None:\n",
    "            #captured background image\n",
    "            bg_im0=cv2.imread(os.path.join(data_path, filename.replace('_img','_back'))); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB);\n",
    "\n",
    "        #segmentation mask\n",
    "        rcnn = cv2.imread(os.path.join(data_path, filename.replace('_img','_masksDL')),0);\n",
    "\n",
    "        if is_video: #if video mode, load target background frames\n",
    "            #target background path\n",
    "            # back_img10=cv2.imread(os.path.join(args.target_back,filename.replace('_img.png','.png'))); back_img10=cv2.cvtColor(back_img10,cv2.COLOR_BGR2RGB);\n",
    "            #Green-screen background\n",
    "            back_img20=np.zeros(bgr_img.shape); back_img20[...,0]=120; back_img20[...,1]=255; back_img20[...,2]=155;\n",
    "\n",
    "            #create multiple frames with adjoining frames\n",
    "            gap=20\n",
    "            multi_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4))\n",
    "            idx=[i-2*gap,i-gap,i+gap,i+2*gap]\n",
    "            for t in range(0,4):\n",
    "                if idx[t]<0:\n",
    "                    idx[t]=len(test_imgs)+idx[t]\n",
    "                elif idx[t]>=len(test_imgs):\n",
    "                    idx[t]=idx[t]-len(test_imgs)\n",
    "\n",
    "                file_tmp=test_imgs[idx[t]]\n",
    "                bgr_img_mul = cv2.imread(os.path.join(data_path, file_tmp));\n",
    "                multi_fr_w[...,t]=cv2.cvtColor(bgr_img_mul,cv2.COLOR_BGR2GRAY);\n",
    "\n",
    "        else:\n",
    "            ## create the multi-frame\n",
    "            multi_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4))\n",
    "            multi_fr_w[...,0] = cv2.cvtColor(bgr_img,cv2.COLOR_BGR2GRAY);\n",
    "            multi_fr_w[...,1] = multi_fr_w[...,0]\n",
    "            multi_fr_w[...,2] = multi_fr_w[...,0]\n",
    "            multi_fr_w[...,3] = multi_fr_w[...,0]\n",
    "\n",
    "            \n",
    "        #crop tightly\n",
    "        bgr_img0=bgr_img;\n",
    "        isBlack= (cv2.countNonZero(rcnn)==0)\n",
    "        if(cv2.countNonZero(rcnn)>15):#Edited\n",
    "            #counter of processed frames\n",
    "            count+=1\n",
    "\n",
    "            bbox=get_bbox(rcnn,R=bgr_img0.shape[0],C=bgr_img0.shape[1])\n",
    "\n",
    "            # crop_list=[bgr_img,bg_im0,rcnn,back_img10,back_img20,multi_fr_w]\n",
    "            crop_list=[bgr_img,bg_im0,rcnn,back_img20,multi_fr_w]\n",
    "            crop_list=crop_images(crop_list,reso,bbox)\n",
    "            bgr_img=crop_list[0]; bg_im=crop_list[1]; rcnn=crop_list[2]; back_img2=crop_list[3]; multi_fr=crop_list[4]\n",
    "            # bgr_img=crop_list[0]; bg_im=crop_list[1]; rcnn=crop_list[2]; back_img1=crop_list[3]; back_img2=crop_list[4]; multi_fr=crop_list[5]\n",
    "\n",
    "            #process segmentation mask\n",
    "            kernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "            kernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "            rcnn=rcnn.astype(np.float32)/255; rcnn[rcnn>0.2]=1;\n",
    "            K=25\n",
    "\n",
    "            zero_id=np.nonzero(np.sum(rcnn,axis=1)==0)\n",
    "            del_id=zero_id[0][zero_id[0]>250]\n",
    "            if len(del_id)>0:\n",
    "                del_id=[del_id[0]-2,del_id[0]-1,*del_id]\n",
    "                rcnn=np.delete(rcnn,del_id,0)\n",
    "            rcnn = cv2.copyMakeBorder( rcnn, 0, K + len(del_id), 0, 0, cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "            rcnn = cv2.erode(rcnn, kernel_er, iterations=10)\n",
    "            rcnn = cv2.dilate(rcnn, kernel_dil, iterations=5)\n",
    "            rcnn=cv2.GaussianBlur(rcnn.astype(np.float32),(31,31),0)\n",
    "            rcnn=(255*rcnn).astype(np.uint8)\n",
    "            rcnn=np.delete(rcnn, range(reso[0],reso[0]+K), 0)\n",
    "\n",
    "\n",
    "            #convert to torch\n",
    "            img=torch.from_numpy(bgr_img.transpose((2, 0, 1))).unsqueeze(0); img=2*img.float().div(255)-1\n",
    "            bg=torch.from_numpy(bg_im.transpose((2, 0, 1))).unsqueeze(0); bg=2*bg.float().div(255)-1\n",
    "            rcnn_al=torch.from_numpy(rcnn).unsqueeze(0).unsqueeze(0); rcnn_al=2*rcnn_al.float().div(255)-1\n",
    "            multi_fr=torch.from_numpy(multi_fr.transpose((2, 0, 1))).unsqueeze(0); multi_fr=2*multi_fr.float().div(255)-1\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img,bg,rcnn_al, multi_fr =Variable(img.cuda()),  Variable(bg.cuda()), Variable(rcnn_al.cuda()), Variable(multi_fr.cuda())\n",
    "                input_im=torch.cat([img,bg,rcnn_al,multi_fr],dim=1)\n",
    "                \n",
    "                alpha_pred,fg_pred_tmp=netM(img,bg,rcnn_al,multi_fr)\n",
    "                \n",
    "                al_mask=(alpha_pred>0.95).type(torch.cuda.FloatTensor)\n",
    "\n",
    "                # for regions with alpha>0.95, simply use the image as fg\n",
    "                fg_pred=img*al_mask + fg_pred_tmp*(1-al_mask)\n",
    "\n",
    "                alpha_out=to_image(alpha_pred[0,...]); \n",
    "\n",
    "                #refine alpha with connected component\n",
    "                labels=label((alpha_out>0.05).astype(int))\n",
    "                try:\n",
    "                    assert( labels.max() != 0 )\n",
    "                except:\n",
    "                    continue\n",
    "                largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "                alpha_out=alpha_out*largestCC\n",
    "\n",
    "                alpha_out=(255*alpha_out[...,0]).astype(np.uint8)                \n",
    "\n",
    "                fg_out=to_image(fg_pred[0,...]); fg_out=fg_out*np.expand_dims((alpha_out.astype(float)/255>0.01).astype(float),axis=2); fg_out=(255*fg_out).astype(np.uint8)\n",
    "\n",
    "                #Uncrop\n",
    "                R0=bgr_img0.shape[0];C0=bgr_img0.shape[1]\n",
    "                alpha_out0=uncrop(alpha_out,bbox,R0,C0)\n",
    "                fg_out0=uncrop(fg_out,bbox,R0,C0)\n",
    "\n",
    "                #it was down else before to include both states (*)\n",
    "                comp_im_tr2=composite4(fg_out0,back_img20,alpha_out0)\n",
    "\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_out'), alpha_out0)\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_fg'), cv2.cvtColor(fg_out0,cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(result_path+'/'+filename.replace('_img','_compose'), cv2.cvtColor(comp_im_tr1,cv2.COLOR_BGR2RGB))\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_matte').format(i), cv2.cvtColor(comp_im_tr2,cv2.COLOR_BGR2RGB))\n",
    "        else:#(*)\n",
    "            alpha_out0=rcnn\n",
    "            fg_out0=bgr_img0\n",
    "            \n",
    "            if(include_empty):\n",
    "                comp_im_tr2=composite4(fg_out0,back_img20,alpha_out0)\n",
    "\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_out'), alpha_out0)\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_fg'), cv2.cvtColor(fg_out0,cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(result_path+'/'+filename.replace('_img','_compose'), cv2.cvtColor(comp_im_tr1,cv2.COLOR_BGR2RGB))\n",
    "                cv2.imwrite(result_path+'/'+filename.replace('_img','_matte').format(i), cv2.cvtColor(comp_im_tr2,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "        #compose\n",
    "        # back_img10=cv2.resize(back_img10,(C0,R0)); back_img20=cv2.resize(back_img20,(C0,R0))\n",
    "        # comp_im_tr1=composite4(fg_out0,back_img10,alpha_out0)\n",
    "\n",
    "        # print(\"fg_out0\",fg_out0.shape)\n",
    "        # print(\"back_img20\",back_img20.shape)\n",
    "        # print(\"alpha_out0\",alpha_out0.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        sys.stdout.write('\\r'+  \"Done: {}/{}    Frames: {}/{}\".format(i+1,len(test_imgs), count, total_frames))\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.path.join(workspace,'teaser_matte.mp4')\n",
    "def images2video(images_folder, video_folder):\n",
    "    FFMPEG_BIN = \"ffmpeg\"\n",
    "    command = [ FFMPEG_BIN,\n",
    "                '-f', \"image2\",\n",
    "                '-r', '30',\n",
    "                '-i', os.path.join(images_folder,\"%04d_matte.png\"),\n",
    "                '-vcodec', 'libx264',\n",
    "                '-crf', '15',\n",
    "                '-pix_fmt', 'yuv420p' ,\n",
    "                video_folder]\n",
    "    print(\"Extracting frames from /output folder...\")\n",
    "    sp.run(command)\n",
    "    print(\"Extraction finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dic(json_var,filename):\n",
    "  with open(filename, 'w') as json_file:\n",
    "    json.dump(json_var, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "def generate_video(original_folder,processed_folder, media_folder):\n",
    "    out_images = glob.glob(os.path.join(processed_folder,'*_out.png'))\n",
    "    pattern = 'output/(.*?)_out'\n",
    "    frame_list = [re.search(pattern, name).group(1) for name in out_images]\n",
    "    frame_list.sort()\n",
    "\n",
    "    video_data = {}\n",
    "    video_data[os.path.join(media_folder, \"trimmed.mp4\")] = os.path.join(original_folder, \"{}_img.png\")##Check the keys\n",
    "    video_data[os.path.join(media_folder,\"masks.mp4\")] = os.path.join(original_folder, \"{}_masksDL.png\")\n",
    "    video_data[os.path.join(media_folder,\"out.mp4\")] = os.path.join(processed_folder, \"{}_out.png\")\n",
    "    video_data[os.path.join(media_folder,\"fg.mp4\")] = os.path.join(processed_folder, \"{}_fg.png\")\n",
    "    video_data[os.path.join(media_folder,\"matte.mp4\")] = os.path.join(processed_folder, \"{}_matte.png\")\n",
    "    for v in video_data:\n",
    "        print('\\ngenerating ', v)\n",
    "        img_array = []\n",
    "        vid_images = [video_data[v].format(f) for f in frame_list]\n",
    "        \n",
    "\n",
    "        for filename in vid_images:\n",
    "            img = cv2.imread(filename)\n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            img_array.append(img)\n",
    "        \n",
    "        out = cv2.VideoWriter(v,cv2.VideoWriter_fourcc(*'mp4v'), 30, size)\n",
    "        \n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "            sys.stdout.write('\\r'+  \"Done: {}/{}\".format(i+1,len(img_array)))\n",
    "            sys.stdout.flush()\n",
    "        out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def process_video(workspace, trained_model, sizes, input_video, back_folder, desired_frames, results_folder):\n",
    "    w_time = {}\n",
    "    for size in sizes:\n",
    "        print(\"\\n\\n Folder:\",size)\n",
    "        size_str = '{}x{}'.format(size[0], size[1])    \n",
    "        w_time[size_str] = []\n",
    "\n",
    "        # Creating folder paths\n",
    "        input_folder = os.path.join(workspace,size_str,\"input\")\n",
    "        output_folder = os.path.join(workspace,size_str, \"output\")\n",
    "\n",
    "        # Creating the folders\n",
    "        size_folder = os.path.join(workspace,size_str)\n",
    "        res_size_folder = os.path.join(results_folder, size_str)\n",
    "        \n",
    "        # Creating the background image path\n",
    "        pattern = 'dataset/(.*?)/1'\n",
    "        subject = int(re.search(pattern, input_video).group(1))\n",
    "        back_path = '{}/{}_{}.png'.format(back_folder,subject,size_str)\n",
    "\n",
    "        folders = [workspace, size_folder, input_folder, output_folder, results_folder, res_size_folder]\n",
    "        for folder in folders:\n",
    "            if(os.path.exists(folder)):\n",
    "                print(\"Folder '{}'\\talready exists, skipping ...\".format(folder))\n",
    "            else:\n",
    "                print(\"Folder '{}'\\tnot found, creating one ...\".format(folder))\n",
    "                os.mkdir(folder)\n",
    "        print(\"Folders succesfully created\")\n",
    "\n",
    "        #converting videos into images\n",
    "        video2image(input_video, size, input_folder)\n",
    "        #Segmentating the video\n",
    "        start = time.perf_counter()\n",
    "        deeplab_segmentation(input_folder, include_empty=False, total_frames = desired_frames, back_img_path=back_path)        \n",
    "        # process = multiprocessing.Process(deeplab_segmentation(input_folder))\n",
    "        # process.start()\n",
    "        # process.join()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        w_time[size_str].append(end-start)\n",
    "        print(\"finished\")\n",
    "        #Cleaning the tensorflow allocated memory\n",
    "\n",
    "        start2 = time.perf_counter()\n",
    "        \n",
    "        obtain_matting(input_folder, output_folder, back_path, trained_model, include_empty=False, total_frames = desired_frames)\n",
    "        \n",
    "\n",
    "        # process = multiprocessing.Process(obtain_matting(input_folder, output_folder, back_path, trained_model))\n",
    "        # process.start()\n",
    "        # process.join()\n",
    "\n",
    "        end2 = time.perf_counter()\n",
    "        #Getting the video matte\n",
    "        # start = time.time()\n",
    "        # obtain_matting(input_folder, output_folder, back_path, trained_model)\n",
    "        # end = time.time()\n",
    "        \n",
    "        w_time[size_str].append(end2-start2)\n",
    "        generate_video(input_folder,output_folder,res_size_folder)\n",
    "        shutil.rmtree(size_folder)\n",
    "    return(w_time)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example of use\n",
    "```python\n",
    "sizes = [(1280,720)]#If you want to do it to multiple sizes put the size in a tuple (w,h)\n",
    "time_dic = {}\n",
    "desired_frames = 300\n",
    "# Model\n",
    "trained_model = \"real-hand-held\"\n",
    "# Folder that contains all the inputs and where the output will be saved\n",
    "workspace = \"walking_clips/temp\"\n",
    "# Base video\n",
    "input_video = \"walking_clips/mario.mp4\"\n",
    "# Background image\n",
    "back_path = \"walking_clips\"\n",
    "# Results folder\n",
    "results_folder = \"walking_clips/results\"\n",
    "process_video(workspace, trained_model, sizes, input_video, back_path, desired_frames, results_folder)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Folder '../../github_media/background_substraction/results'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/back_images'\talready exists, skipping ...\n\n------------\nSubject 27\n------------\n\n\n\n Folder: (1280, 720)\nFolder '../../github_media/background_substraction/temp'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/temp/1280x720'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/temp/1280x720/input'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/temp/1280x720/output'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/results/27'\talready exists, skipping ...\nFolder '../../github_media/background_substraction/results/27/1280x720'\talready exists, skipping ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 135/3306    Frames: 127/300\tbackground image extracted\nDone: 423/3306    Frames: 300/300\nDone: ../../github_media/background_substraction/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  ../../github_media/background_substraction/results/27/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  ../../github_media/background_substraction/results/27/1280x720/masks.mp4\nDone: 300/300\ngenerating  ../../github_media/background_substraction/results/27/1280x720/out.mp4\nDone: 300/300\ngenerating  ../../github_media/background_substraction/results/27/1280x720/fg.mp4\nDone: 300/300\ngenerating  ../../github_media/background_substraction/results/27/1280x720/matte.mp4\nDone: 300/300"
    }
   ],
   "source": [
    "# Model\n",
    "trained_model = \"real-hand-held\"\n",
    "# Sizes to be processed\n",
    "sizes = [(1280,720)]#If you want to do it to multiple sizes put the size in a tuple (w,h)\n",
    "# Dictionary where times will be stored\n",
    "time_dic = {}\n",
    "# Loading all the video paths\n",
    "video_list = glob.glob('/home/josmar/proyectos/codes/datasets/ucb_gait/dataset/**/1.mp4',recursive=True)\n",
    "video_list.sort()\n",
    "\n",
    "# Number of frames at output\n",
    "desired_frames = 300\n",
    "# Folder that will contain the results\n",
    "results_folder='../../github_media/background_substraction/results'\n",
    "# Folder that will contain the background images\n",
    "back_folder = '../../github_media/background_substraction/back_images'\n",
    "#Checking if results_folder exists, create one if it doesnt\n",
    "for folder in [results_folder,back_folder]:\n",
    "    if(os.path.exists(folder)):\n",
    "        print(\"Folder '{}'\\talready exists, skipping ...\".format(folder))\n",
    "    else:\n",
    "        print(\"Folder '{}'\\tnot found, creating one ...\".format(folder))\n",
    "        os.mkdir(folder)\n",
    "# temp file where images will be saved\n",
    "    workspace = \"../../github_media/background_substraction/temp\"\n",
    "\n",
    "for input_video in video_list[26:27]:\n",
    "\n",
    "    # Each element of video_list is a input_video\n",
    "    #Finding the subject id\n",
    "    pattern = 'dataset/(.*?)/1'\n",
    "    subject = int(re.search(pattern, input_video).group(1))\n",
    "    print('\\n------------\\nSubject {}\\n------------\\n'.format(subject))\n",
    "    #Naming a result folder for the subject\n",
    "    sub_res_folder = os.path.join(results_folder,str(subject))\n",
    "    try:\n",
    "        time_dic[subject] = process_video(workspace, trained_model, sizes, input_video, back_folder, desired_frames, sub_res_folder)\n",
    "    finally:\n",
    "        save_dic(time_dic,'../../github_media/background_substraction/times.json')\n",
    "    save_dic(time_dic,'../../github_media/background_substraction/times.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1080: [547.811765909195, 550.4250721931458]\n",
    "\n",
    "{\"1920x1080\": [642.9244840145111, 1109.3847675323486]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Folder 'walking_clips/results'\talready exists, skipping ...\nFolder 'walking_clips/back_images'\talready exists, skipping ...\n\n------------\nSubject 41\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/41'\tnot found, creating one ...\nFolder 'walking_clips/results/41/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 153/3938    Frames: 145/300\tbackground image extracted\nDone: 369/3938    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/41/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/41/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/41/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/41/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/41/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 42\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/42'\tnot found, creating one ...\nFolder 'walking_clips/results/42/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 139/3563    Frames: 131/300\tbackground image extracted\nDone: 476/3563    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/42/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/42/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/42/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/42/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/42/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 43\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/43'\tnot found, creating one ...\nFolder 'walking_clips/results/43/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 133/4372    Frames: 125/300\tbackground image extracted\nDone: 565/4372    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/43/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/43/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/43/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/43/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/43/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 44\n------------\n\n\n\n Folder:(1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/44'\tnot found, creating one ...\nFolder 'walking_clips/results/44/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 132/3454    Frames: 124/300\tbackground image extracted\nDone: 611/3454    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/44/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/44/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/44/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/44/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/44/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 45\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/45'\tnot found, creating one ...\nFolder 'walking_clips/results/45/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 123/3428    Frames: 115/300\tbackground image extracted\nDone: 592/3428    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/45/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/45/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/45/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/45/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/45/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 46\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/46'\tnot found, creating one ...\nFolder 'walking_clips/results/46/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 136/2877    Frames: 128/300\tbackground image extracted\nDone: 433/2877    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/46/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/46/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/46/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/46/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/46/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 47\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/47'\tnot found, creating one ...\nFolder 'walking_clips/results/47/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 165/3242    Frames: 157/300\tbackground image extracted\nDone: 433/3242    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/47/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/47/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/47/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/47/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/47/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 48\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/48'\tnot found, creating one ...\nFolder 'walking_clips/results/48/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 8/3836    Frames: 0/300\tbackground image extracted\nDone: 715/3836    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/48/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/48/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/48/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/48/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/48/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 49\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/49'\tnot found, creating one ...\nFolder 'walking_clips/results/49/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 178/3232    Frames: 170/300\tbackground image extracted\nDone: 414/3232    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/49/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/49/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/49/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/49/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/49/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 50\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/50'\tnot found, creating one ...\nFolder 'walking_clips/results/50/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 167/3445    Frames: 159/300\tbackground image extracted\nDone: 413/3445    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/50/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/50/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/50/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/50/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/50/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 51\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/51'\tnot found, creating one ...\nFolder 'walking_clips/results/51/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 184/3479    Frames: 176/300\tbackground image extracted\nDone: 431/3479    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/51/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/51/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/51/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/51/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/51/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 52\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/52'\tnot found, creating one ...\nFolder 'walking_clips/results/52/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 8/2828    Frames: 0/300\tbackground image extracted\nDone: 621/2828    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/52/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/52/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/52/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/52/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/52/1280x720/matte.mp4\nDone: 300/300\n------------\nSubject 53\n------------\n\n\n\n Folder: (1280, 720)\nFolder 'walking_clips/temp'\talready exists, skipping ...\nFolder 'walking_clips/temp/1280x720'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/input'\tnot found, creating one ...\nFolder 'walking_clips/temp/1280x720/output'\tnot found, creating one ...\nFolder 'walking_clips/results/53'\tnot found, creating one ...\nFolder 'walking_clips/results/53/1280x720'\tnot found, creating one ...\nFolders succesfully created\nExtracting frames from video into /input folder...\nExtraction finished successfully\nmodel loaded successfully!\nDone: 8/2717    Frames: 0/300\tbackground image extracted\nDone: 810/2717    Frames: 300/300\nDone: walking_clips/temp/1280x720/input\nfinished\nCUDA Device: 0,1\nUsing video mode\nDone: 300/300    Frames: 300/300\ngenerating  walking_clips/results/53/1280x720/trimmed.mp4\nDone: 300/300\ngenerating  walking_clips/results/53/1280x720/masks.mp4\nDone: 300/300\ngenerating  walking_clips/results/53/1280x720/out.mp4\nDone: 300/300\ngenerating  walking_clips/results/53/1280x720/fg.mp4\nDone: 300/300\ngenerating  walking_clips/results/53/1280x720/matte.mp4\nDone: 300/300"
    }
   ],
   "source": [
    "# Model\n",
    "trained_model = \"real-hand-held\"\n",
    "# Sizes to be processed\n",
    "sizes = [(1280,720)]#If you want to do it to multiple sizes put the size in a tuple (w,h)\n",
    "# Dictionary where times will be stored\n",
    "time_dic = {}\n",
    "# Loading all the video paths\n",
    "video_list = glob.glob('/home/josmar/proyectos/codes/datasets/ucb_gait/dataset/**/1.mp4',recursive=True)\n",
    "video_list.sort()\n",
    "\n",
    "# Number of frames at output\n",
    "desired_frames = 300\n",
    "# Folder that will contain the results\n",
    "results_folder='walking_clips/results'\n",
    "# Folder that will contain the background images\n",
    "back_folder = 'walking_clips/back_images'\n",
    "#Checking if results_folder exists, create one if it doesnt\n",
    "for folder in [results_folder,back_folder]:\n",
    "    if(os.path.exists(folder)):\n",
    "        print(\"Folder '{}'\\talready exists, skipping ...\".format(folder))\n",
    "    else:\n",
    "        print(\"Folder '{}'\\tnot found, creating one ...\".format(folder))\n",
    "        os.mkdir(folder)\n",
    "# temp file where images will be saved\n",
    "    workspace = \"walking_clips/temp\"\n",
    "\n",
    "for input_video in video_list[40:53]:\n",
    "\n",
    "    # Each element of video_list is a input_video\n",
    "    #Finding the subject id\n",
    "    pattern = 'dataset/(.*?)/1'\n",
    "    subject = int(re.search(pattern, input_video).group(1))\n",
    "    print('\\n------------\\nSubject {}\\n------------\\n'.format(subject))\n",
    "    #Naming a result folder for the subject\n",
    "    sub_res_folder = os.path.join(results_folder,str(subject))\n",
    "    try:\n",
    "        time_dic[subject] = process_video(workspace, trained_model, sizes, input_video, back_folder, desired_frames, sub_res_folder)\n",
    "    finally:\n",
    "        save_dic(time_dic,'walking_clips/times.json')\n",
    "    save_dic(time_dic,'walking_clips/times.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "out_images = glob.glob('walking_clips/1280x720/output/*_out.png')\n",
    "print(out_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n"
    }
   ],
   "source": [
    "video_list = glob.glob('/home/josmar/proyectos/codes/datasets/ucb_gait/dataset/**/1.mp4',recursive=True)\n",
    "video_list.sort()\n",
    "pattern = 'dataset/(.*?)/1'\n",
    "subject_list = [int(re.search(pattern, name).group(1)) for name in video_list]\n",
    "print (subject_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/josmar/proyectos/codes/datasets/ucb_gait/dataset/01/1.mp4\n/home/josmar/proyectos/codes/datasets/ucb_gait/dataset/02/1.mp4\n"
    }
   ],
   "source": [
    "for input_video in video_list[:2]:\n",
    "    print(input_video)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600559096958",
   "display_name": "Python 3.6.10 64-bit ('backmat': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}