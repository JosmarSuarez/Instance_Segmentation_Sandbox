{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "import shutil\n",
    "\n",
    "# Segmentation dependencies\n",
    "import sys\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2, pdb, glob, argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#importing numba to clean all memory allocated\n",
    "from numba import cuda\n",
    "\n",
    "#Matting dependencies\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "import glob, time, argparse, pdb\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.measure import label\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from functions import *\n",
    "from networks import ResnetConditionHR\n",
    "\n",
    "# Import time to measure time of execution\n",
    "import time\n",
    "\n",
    "# Import json to write times\n",
    "import json\n",
    "#Tensorflow doesnt release memory after executed(NO used)\n",
    "import multiprocessing\n",
    "import re\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Separating video in images\n",
    "\n",
    "# %%\n",
    "def video2image(input_video, size, input_folder):\n",
    "    FFMPEG_BIN = \"ffmpeg\"\n",
    "    command = [ FFMPEG_BIN,\n",
    "                '-i', input_video, \n",
    "                '-vf', 'scale={}:{}'.format(size[0],size[1]),\n",
    "                os.path.join(input_folder,\"%04d_img.png\")]\n",
    "    print(\"Extracting frames from video into /input folder...\")\n",
    "    sp.run(command)\n",
    "    print(\"Extraction finished successfully\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Running segmentation\n",
    "# Using deeplabv3 to segment the images\n",
    "\n",
    "# %%\n",
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        #\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "\n",
    "        tar_file.close()\n",
    "\n",
    "        if graph_def is None:\n",
    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "        #I added this code\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        #----------------\n",
    "        self.sess = tf.Session(config=config,graph=self.graph)\n",
    "\n",
    "    def run(self, image):\n",
    "        \"\"\"Runs inference on a single image.\n",
    "\n",
    "        Args:\n",
    "          image: A PIL.Image object, raw input image.\n",
    "\n",
    "        Returns:\n",
    "          resized_image: RGB image resized from original input image.\n",
    "          seg_map: Segmentation map of `resized_image`.\n",
    "        \"\"\"\n",
    "        width, height = image.size\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return resized_image, seg_map\n",
    "\n",
    "\n",
    "# %%\n",
    "def create_pascal_label_colormap():\n",
    "    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
    "\n",
    "    Returns:\n",
    "    A Colormap for visualizing segmentation results.\n",
    "    \"\"\"\n",
    "    colormap = np.zeros((256, 3), dtype=int)\n",
    "    ind = np.arange(256, dtype=int)\n",
    "\n",
    "    for shift in reversed(range(8)):\n",
    "        for channel in range(3):\n",
    "          colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "        ind >>= 3\n",
    "\n",
    "    return colormap\n",
    "\n",
    "def label_to_color_image(label):\n",
    "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "    Args:\n",
    "    label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "    Returns:\n",
    "    result: A 2D array with floating type. The element of the array\n",
    "      is the color indexed by the corresponding element in the input label\n",
    "      to the PASCAL color map.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If label is not of rank 2 or its value is larger than color\n",
    "      map maximum entry.\n",
    "    \"\"\"\n",
    "    if label.ndim != 2:\n",
    "        raise ValueError('Expect 2-D input label')\n",
    "\n",
    "    colormap = create_pascal_label_colormap()\n",
    "\n",
    "    if np.max(label) >= len(colormap):\n",
    "        raise ValueError('label value too large.')\n",
    "\n",
    "    return colormap[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model loaded successfully!\n"
    }
   ],
   "source": [
    "\n",
    "## setup ####################\n",
    "\n",
    "LABEL_NAMES = np.asarray([\n",
    "        'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "        'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "        'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "])\n",
    "\n",
    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
    "\n",
    "\n",
    "MODEL_NAME = 'xception_coco_voctrainval'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n",
    "\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "_MODEL_URLS = {\n",
    "        'mobilenetv2_coco_voctrainaug':\n",
    "            'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "        'mobilenetv2_coco_voctrainval':\n",
    "            'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
    "        'xception_coco_voctrainaug':\n",
    "            'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
    "        'xception_coco_voctrainval':\n",
    "            'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
    "}\n",
    "_TARBALL_NAME = _MODEL_URLS[MODEL_NAME]\n",
    "\n",
    "model_dir = 'deeplab_model'\n",
    "if not os.path.exists(model_dir):\n",
    "    tf.gfile.MakeDirs(model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
    "if not os.path.exists(download_path):\n",
    "    print('downloading model to %s, this might take a while...' % download_path)\n",
    "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], \n",
    "                        download_path)\n",
    "    print('download completed! loading DeepLab model...')\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')\n",
    "\n",
    "#######################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0  0  0 ...  0  0  0]\n [ 0  0  0 ...  0  0  0]\n [ 0  0  0 ...  0  0  0]\n ...\n [18 18 18 ...  0  0  0]\n [ 0 18 18 ...  0  0  0]\n [ 0  0  0 ...  0  0  0]]\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 1 5 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "image = Image.open('/home/josmar/proyectos/codes/annotation_tools/background_substraction/interior1/1280x720/input/0001_img.png')\n",
    "res_im,seg=MODEL.run(image)\n",
    "print(seg)\n",
    "seg=cv2.resize(seg.astype(np.uint8),image.size)\n",
    "print(seg)\n",
    "mask_sel=(seg==15).astype(np.float32)\n",
    "out_mask = (255*mask_sel).astype(np.uint8)\n",
    "\n",
    "\n",
    "img = cv2.imread('/home/josmar/proyectos/codes/annotation_tools/background_substraction/interior1/1280x720/input/0001_masksDL.png',0)\n",
    "print(type(out_mask))\n",
    "print(type(img))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5),np.uint8)\n",
    "erosion = cv2.erode(img,kernel,iterations = 1)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "opening = cv2.morphologyEx(out_mask, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('image',opening)\n",
    "# cv2.imshow('opening',opening)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "binary_images/2_720_correction_close.mp4 was successfully saved\n"
    }
   ],
   "source": [
    "def modify_video(folder_path,vid_name, out_folder,kernel,th_low=150,th_up=255):    \n",
    "    \n",
    "\n",
    "    complete_path = os.path.join(folder_path,vid_name)\n",
    "\n",
    "    \n",
    "    # Python program to save a  \n",
    "    # video using OpenCV \n",
    "    \n",
    "    \n",
    "    import cv2 \n",
    "    \n",
    "    \n",
    "    # Create an object to read  \n",
    "    # from camera \n",
    "    video = cv2.VideoCapture(complete_path) \n",
    "    \n",
    "    # We need to check if camera \n",
    "    # is opened previously or not \n",
    "    if (video.isOpened() == False):  \n",
    "        print(\"Error reading video file\") \n",
    "    \n",
    "    # We need to set resolutions. \n",
    "    # so, convert them from float to integer. \n",
    "    frame_width = int(video.get(3)) \n",
    "    frame_height = int(video.get(4)) \n",
    "    \n",
    "    size = (frame_width, frame_height) \n",
    "    \n",
    "    # Below VideoWriter object will create \n",
    "    # a frame of above defined The output  \n",
    "    # is stored in 'filename.avi' file.\n",
    "    new_name = '{}_close.mp4'.format(vid_name.split('.')[0])\n",
    "    new_name = os.path.join(out_folder,new_name)\n",
    "    result = cv2.VideoWriter(new_name,  \n",
    "                            cv2.VideoWriter_fourcc(*'MP4V'), \n",
    "                            30, size) \n",
    "        \n",
    "    while(True): \n",
    "        ret, frame = video.read() \n",
    "    \n",
    "        if ret == True: \n",
    "            _,thresh = cv2.threshold(frame,th_low,th_up,cv2.THRESH_BINARY)\n",
    "\n",
    "            opening = cv2.morphologyEx(frame, cv2.MORPH_OPEN, kernel)\n",
    "            close = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "            # Write the frame into the \n",
    "            # file 'filename.avi' \n",
    "            result.write(close) \n",
    "    \n",
    "            # Display the frame \n",
    "            # saved in the file \n",
    "            # cv2.imshow('Frame', close) \n",
    "\n",
    "            # Press S on keyboard  \n",
    "            # to stop the process \n",
    "            if cv2.waitKey(1) & 0xFF == ord('s'): \n",
    "                break\n",
    "    \n",
    "        # Break the loop \n",
    "        else: \n",
    "            break\n",
    "    \n",
    "    # When everything done, release  \n",
    "    # the video capture and video  \n",
    "    # write objects \n",
    "    video.release() \n",
    "    result.release() \n",
    "        \n",
    "    # Closes all the frames \n",
    "    cv2.destroyAllWindows() \n",
    "    \n",
    "    print(\"{} was successfully saved\".format(new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bin_close_images/10_720_correction_close.mp4 was successfully saved\nbin_close_images/12_720_correction_close.mp4 was successfully saved\nbin_close_images/13_720_correction_close.mp4 was successfully saved\nbin_close_images/14_720_correction_close.mp4 was successfully saved\nbin_close_images/15_720_correction_close.mp4 was successfully saved\nbin_close_images/16_720_correction_close.mp4 was successfully saved\nbin_close_images/18_720_correction_close.mp4 was successfully saved\nbin_close_images/19_720_correction_close.mp4 was successfully saved\nbin_close_images/20_720_correction_close.mp4 was successfully saved\nbin_close_images/22_720_correction_close.mp4 was successfully saved\nbin_close_images/23_720_correction_close.mp4 was successfully saved\nbin_close_images/24_720_correction_close.mp4 was successfully saved\nbin_close_images/25_720_correction_close.mp4 was successfully saved\nbin_close_images/26_720_correction_close.mp4 was successfully saved\nbin_close_images/27_720_correction_close.mp4 was successfully saved\nbin_close_images/29_720_correction_close.mp4 was successfully saved\nbin_close_images/2_720_correction_close.mp4 was successfully saved\nbin_close_images/30_720_correction_close.mp4 was successfully saved\nbin_close_images/31_720_correction_close.mp4 was successfully saved\nbin_close_images/32_720_correction_close.mp4 was successfully saved\nbin_close_images/34_720_correction_close.mp4 was successfully saved\nbin_close_images/35_720_correction_close.mp4 was successfully saved\nbin_close_images/36_720_correction_close.mp4 was successfully saved\nbin_close_images/37_720_correction_close.mp4 was successfully saved\nbin_close_images/39_720_correction_close.mp4 was successfully saved\nbin_close_images/3_720_correction_close.mp4 was successfully saved\nbin_close_images/40_720_correction_close.mp4 was successfully saved\nbin_close_images/43_720_correction_close.mp4 was successfully saved\nbin_close_images/44_720_correction_close.mp4 was successfully saved\nbin_close_images/45_720_correction_close.mp4 was successfully saved\nbin_close_images/46_720_correction_close.mp4 was successfully saved\nbin_close_images/47_720_correction_close.mp4 was successfully saved\nbin_close_images/4_720_correction_close.mp4 was successfully saved\nbin_close_images/50_720_correction_close.mp4 was successfully saved\nbin_close_images/52_720_correction_close.mp4 was successfully saved\nbin_close_images/53_720_correction_close.mp4 was successfully saved\nbin_close_images/5_720_correction_close.mp4 was successfully saved\nbin_close_images/6_720_correction_close.mp4 was successfully saved\nbin_close_images/7_720_correction_close.mp4 was successfully saved\nbin_close_images/8_720_correction_close.mp4 was successfully saved\nbin_close_images/9_720_correction_close.mp4 was successfully saved\n"
    }
   ],
   "source": [
    "input_folder = '/home/josmar/proyectos/Background_Matting/walking_clips/krita_files'\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "th_low = 100\n",
    "th_up = 255\n",
    "out_folder = 'bin_close_images'\n",
    "video_list = os.listdir(input_folder)\n",
    "video_list.sort()\n",
    "for v in video_list:\n",
    "    modify_video(input_folder, v, out_folder, kernel, th_low, th_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "more_close/27_720_correction_close.mp4 was successfully saved\n"
    }
   ],
   "source": [
    "input_folder = '/home/josmar/proyectos/Background_Matting/walking_clips/krita_files'\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "th_low = 150\n",
    "th_up = 255\n",
    "out_folder = 'more_close'\n",
    "video_list = os.listdir(input_folder)\n",
    "video_list.sort()\n",
    "modify_video(input_folder, \"27_720_correction.mp4\", out_folder, kernel, th_low, th_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "78\n0\n"
    }
   ],
   "source": [
    "print(cv2.countNonZero(img))\n",
    "print(cv2.countNonZero(opening))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601563257643",
   "display_name": "Python 3.6.10 64-bit ('backmat': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}